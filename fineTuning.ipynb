{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installs for google colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# installing dependencies\n",
    "!git clone --recursive https://github.com/CPJKU/madmom.git\n",
    "%cd madmom\n",
    "!pip install -e .\n",
    "%pip install https://github.com/CPJKU/beat_this/archive/main.zip\n",
    "%pip install mir-eval pytorch-lightning torchaudio pedalboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")\n",
    "\n",
    "# !cd drive/MyDrive && unzip move.zip -d beatTrackingProject/ && rm move.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports + variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "from pytorch_lightning import Trainer, seed_everything\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "from beat_this.dataset import BeatDataModule\n",
    "from beat_this.model.pl_module import PLBeatThis\n",
    "\n",
    "from pedalboard import PitchShift\n",
    "from pedalboard._pedalboard import Pedalboard\n",
    "\n",
    "args = {\n",
    "    \"name\": \"finetuningOnSalsa\",\n",
    "    \"gpu\": 0,\n",
    "    \"force_flash_attention\": False,\n",
    "    \"compile\": [], #[\"frontend\", \"transformer_blocks\", \"task_heads\"],\n",
    "    \"n_layers\": 6,\n",
    "    \"transformer_dim\": 512,\n",
    "    \"frontend_dropout\": 0.1,\n",
    "    \"transformer_dropout\": 0.2,\n",
    "    \"lr\": 0.0003,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"logger\": \"none\",\n",
    "    \"num_workers\": 8,\n",
    "    \"n_heads\": 16,\n",
    "    \"fps\": 50,\n",
    "    \"loss\": \"shift_tolerant_weighted_bce\", # other options are \"fast_shift_tolerant_weighted_bce\", \"weighted_bce\", \"bce\"\n",
    "    \"warmup_steps\": 100,\n",
    "    \"max_epochs\": 20,\n",
    "    \"batch_size\": 8,\n",
    "    \"accumulate_grad_batches\": 8,\n",
    "    \"train_length\": 1500,\n",
    "    \"dbn\": True,\n",
    "    \"eval_trim_beats\": 5, # skip the first given seconds per piece in evaluating\n",
    "    \"val_frequency\": 2, # validate every N epochs\n",
    "    \"tempo_augmentation\": True,\n",
    "    \"pitch_augmentation\": True,\n",
    "    \"mask_augmentation\": True,\n",
    "    \"sum_head\": True,\n",
    "    \"partial_transformers\": True,\n",
    "    \"length_based_oversampling_factor\": 0.5, # usually 0.65\n",
    "    \"val\": True, # whether to include the validation data in training\n",
    "    \"hung_data\": False,\n",
    "    \"fold\": None,\n",
    "    \"seed\": 127,\n",
    "    \"resume_checkpoint\": \"final0\",\n",
    "    # \"resume_id\": None,\n",
    "}\n",
    "\n",
    "seed_everything(args[\"seed\"], workers = True)\n",
    "\n",
    "params_str = f\"{'noval ' if not args.get('val') else ''}{'hung ' if args.get('hung_data') else ''}{'fold' + str(args.get('fold')) + ' ' if args.get('fold') is not None else ''}{args.get('loss')}-h{args.get('transformer_dim')}-aug{args.get('tempo_augmentation')}{args.get('pitch_augmentation')}{args.get('mask_augmentation')}{' nosumH ' if not args.get('sum_head') else ''}{' nopartialT ' if not args.get('partial_transformers') else ''}\"\n",
    "if args.get('logger') == \"wandb\":\n",
    "    if args.get('resume_checkpoint') and args.get('resume_id'):\n",
    "        wandb_args = dict(id = args.get('resume_id'), resume = \"must\")\n",
    "    else:\n",
    "        wandb_args = {}\n",
    "    logger = WandbLogger(\n",
    "        project = \"beat_this\", name = f\"{args.get('name')} {params_str}\".strip(), **wandb_args\n",
    "    )\n",
    "else:\n",
    "    logger = None\n",
    "\n",
    "# i'm on cpu so no flash attention for me :(\n",
    "if args.get(\"force_flash_attention\"):\n",
    "    print(\"Forcing the use of the flash attention.\")\n",
    "    torch.backends.cuda.enable_flash_sdp(True)\n",
    "    torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "    torch.backends.cuda.enable_math_sdp(False)\n",
    "\n",
    "augmentations = {}\n",
    "# if args.get('tempo_augmentation'):\n",
    "#     augmentations[\"tempo\"] = {\"min\": -20, \"max\": 20, \"stride\": 4}\n",
    "if args.get('pitch_augmentation'):\n",
    "    augmentations[\"pitch\"] = {\"min\": -5, \"max\": 6}\n",
    "if args.get('mask_augmentation'):\n",
    "    # kind, min_count, max_count, min_len, max_len, min_parts, max_parts\n",
    "    augmentations[\"mask\"] = {\n",
    "        \"kind\": \"permute\",\n",
    "        \"min_count\": 1,\n",
    "        \"max_count\": 6,\n",
    "        \"min_len\": 0.1,\n",
    "        \"max_len\": 2,\n",
    "        \"min_parts\": 5,\n",
    "        \"max_parts\": 9,\n",
    "    }\n",
    "\n",
    "# env = \"colab\"\n",
    "env = \"local\"\n",
    "dataDir       = Path(\"/content/drive/MyDrive/beatTrackingProject/data/\") if env == \"colab\" else Path(\"data/\")\n",
    "checkpointDir = (Path(\"/content/drive/MyDrive/beatTrackingProject/checkpoints/\")) if env == \"colab\" else (Path(\"checkpoints/\"))\n",
    "assert dataDir.exists()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Engineering Shenanigans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train + Eval data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmentPitch(audio, samplingRate, semitone, transform, savePath):\n",
    "    if savePath.exists():\n",
    "        return\n",
    "    if semitone == 0:\n",
    "        shiftedAudio = audio.detach().clone()\n",
    "    else:\n",
    "        board = Pedalboard([PitchShift(semitones = semitone)])\n",
    "        shiftedAudio = torch.from_numpy(board(audio.numpy(), samplingRate))  # pyright: ignore[reportArgumentType]\n",
    "    \n",
    "    melSpec = transform(shiftedAudio).squeeze(0).T\n",
    "    melSpec = torch.log(1 + 1000 * melSpec)\n",
    "\n",
    "    np.save(savePath, melSpec.numpy().astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr       = 22050\n",
    "songsDir = dataDir / \"audio/songs\"\n",
    "annotDir = dataDir / \"annotations/origAnnotations/\"\n",
    "\n",
    "TEMPO_CHANGES = np.arange(-20, 21, step = 4)\n",
    "PITCH_CHANGES = np.arange(-5 , 7 , step = 1)\n",
    "\n",
    "melTransform = T.MelSpectrogram(\n",
    "    sample_rate = sr,\n",
    "    n_fft = 1024,\n",
    "    hop_length = 441,\n",
    "    n_mels = 128,\n",
    "    f_min = 30,\n",
    "    f_max = 10000,\n",
    "    power = 1.0,\n",
    "    mel_scale = \"slaney\",\n",
    "    normalized = False\n",
    ")\n",
    "\n",
    "for fragmentPath in tqdm(list(songsDir.iterdir()), desc = \"Processing songs\"):\n",
    "    if fragmentPath.name[-4:] != \".mp3\":\n",
    "        print(f\"File {fragmentPath.name} is not a .mp3 song, skipping.\")\n",
    "        continue\n",
    "    \n",
    "    N            = int(fragmentPath.name.split('.')[0])\n",
    "    # saving locations\n",
    "    spectrogramPath = songsDir.parent / \"spectrograms\" / \"salsa\" / f\"song{N}\" / \"track.npy\"\n",
    "    beatsPath       = annotDir.parent / \"salsa\" / \"annotations\" / \"beats\" / f\"song{N}.beats\"\n",
    "    spectrogramPath.parent.mkdir(parents = True, exist_ok = True)\n",
    "    beatsPath.parent.mkdir(      parents = True, exist_ok = True)\n",
    "\n",
    "    song, origSR = torchaudio.load(fragmentPath)\n",
    "    if song.shape[0] > 1:\n",
    "        song = torch.mean(song, dim = 0, keepdim = True)\n",
    "    if origSR != sr:\n",
    "        song = T.Resample(origSR, sr)(song)\n",
    "\n",
    "    for pitch in PITCH_CHANGES:\n",
    "        path = spectrogramPath if pitch == 0 else \\\n",
    "            spectrogramPath.parent / f\"track_ps{pitch}.npy\"\n",
    "        augmentPitch(song, sr, pitch, melTransform, path)\n",
    "    \n",
    "    downbeats = np.loadtxt(annotDir / f\"{N}.txt\") / 1000\n",
    "    upbeats   = (downbeats[1:] + downbeats[:-1]) / 2\n",
    "    allBeats  = np.empty(len(downbeats) + len(upbeats))\n",
    "\n",
    "    allBeats[ ::2] = downbeats\n",
    "    allBeats[1::2] = upbeats\n",
    "\n",
    "    if allBeats[-1] * 2 - allBeats[-2] < song.shape[1] / sr:\n",
    "        allBeats = np.append(allBeats, allBeats[-1] * 2 - allBeats[-2])\n",
    "    \n",
    "    with open(beatsPath, \"w\") as f:\n",
    "        i = 1\n",
    "        for beat in allBeats:\n",
    "            f.write(f\"{beat:.3f}\\t{i}\\n\")\n",
    "            i = (i + 1) % 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unfetchedSongs = Path(\"archive/audioFragments/\")\n",
    "\n",
    "testIndices = []\n",
    "for fragmentPath in tqdm(list(unfetchedSongs.iterdir())):\n",
    "    if fragmentPath.name[-4:] != \".wav\":\n",
    "        print(f\"File {fragmentPath.name} is not a .wav fragment, skipping.\")\n",
    "        continue\n",
    "    if (songsDir / f\"{fragmentPath.stem}.mp3\").exists():\n",
    "        # print(f\"Song {fragmentPath.stem} is already in the train/eval set, skipping.\")\n",
    "        continue\n",
    "    N            = int(fragmentPath.name.split('.')[0])\n",
    "    \n",
    "    # # saving locations\n",
    "    spectrogramPath = songsDir.parent / \"spectrograms\" / \"salsaTest\" / f\"song{N}\" / \"track.npy\"\n",
    "    beatsPath       = annotDir.parent / \"salsaTest\" / \"annotations\" / \"beats\" / f\"song{N}.beats\"\n",
    "    spectrogramPath.parent.mkdir(parents = True, exist_ok = True)\n",
    "    beatsPath.parent.mkdir(      parents = True, exist_ok = True)\n",
    "\n",
    "    # if spectrogramPath.exists() and beatsPath.exists():\n",
    "    #     print(f\"Fragment {N}.wav was already processed, continuing...\\n\")\n",
    "    #     continue\n",
    "\n",
    "    song, origSR = torchaudio.load(fragmentPath)\n",
    "    if song.shape[0] > 1:\n",
    "        song = torch.mean(song, dim = 0, keepdim = True)\n",
    "    if origSR != sr:\n",
    "        song = T.Resample(origSR, sr)(song)\n",
    "\n",
    "    downbeats = np.loadtxt(annotDir / f\"origAnnotations/{N}.txt\") / 1000\n",
    "    downbeats = downbeats[downbeats < song.shape[1] / sr]\n",
    "    upbeats   = (downbeats[1:] + downbeats[:-1]) / 2\n",
    "    allBeats  = np.empty(len(downbeats) + len(upbeats))\n",
    "\n",
    "    allBeats[ ::2] = downbeats\n",
    "    allBeats[1::2] = upbeats\n",
    "\n",
    "    if allBeats[-1] * 2 - allBeats[-2] < song.shape[1] / sr:\n",
    "        allBeats = np.append(allBeats, allBeats[-1] * 2 - allBeats[-2])\n",
    "\n",
    "    melSpec = melTransform(song).squeeze(0).T\n",
    "    melSpec = torch.log(1 + 1000 * melSpec)\n",
    "\n",
    "    np.save(spectrogramPath, melSpec.numpy().astype(np.float32))\n",
    "    with open(beatsPath, \"w\") as f:\n",
    "        i = 1\n",
    "        for beat in allBeats:\n",
    "            f.write(f\"{beat:.3f}\\t{i}\\n\")\n",
    "            i = (i + 1) % 2\n",
    "    \n",
    "    testIndices.append(f\"song{N}\")\n",
    "\n",
    "print(len(testIndices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Saving the train-val splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "existingSongs = ([song.name.split('.')[0] for song in beatsPath.parent.iterdir()])\n",
    "np.random.shuffle(existingSongs)\n",
    "\n",
    "trainPart  = 0.8\n",
    "evalPart   = 0.2\n",
    "totalSongs = len(existingSongs)\n",
    "trainSongs = int(trainPart * totalSongs)\n",
    "\n",
    "i = 0\n",
    "\n",
    "split = list()\n",
    "while i < totalSongs:\n",
    "    part = \"train\" if i < trainSongs else \"val\"\n",
    "    split.append((existingSongs[i], part))\n",
    "    i += 1\n",
    "\n",
    "splitFile = annotDir / \"single.split\"\n",
    "pd.DataFrame(split).to_csv(splitFile, sep = \"\\t\", header = None)  # pyright: ignore[reportCallIssue, reportArgumentType]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beat Data Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule = BeatDataModule(\n",
    "    dataDir,\n",
    "    batch_size      = args['batch_size'],\n",
    "    train_length    = args['train_length'],\n",
    "    spect_fps       = args['fps'],\n",
    "    num_workers     = args['num_workers'],\n",
    "    test_dataset    = \"salsaTest\",\n",
    "    length_based_oversampling_factor = args['length_based_oversampling_factor'],\n",
    "    augmentations   = augmentations,\n",
    "    hung_data       = args['hung_data'],\n",
    "    no_val          = not args['val'],\n",
    "    fold            = args['fold'],\n",
    ")\n",
    "datamodule.setup(stage = \"fit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_weights = datamodule.get_train_positive_weights(widen_target_mask = 3)\n",
    "print(\"Using positive weights: \", pos_weights)\n",
    "dropout = {\n",
    "    \"frontend\": args['frontend_dropout'],\n",
    "    \"transformer\": args['transformer_dropout'],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model loading & training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "if args['resume_checkpoint']:\n",
    "    from beat_this.inference import load_checkpoint #, load_model\n",
    "\n",
    "    print(f\"Loading pretrained checkpoint: {args['resume_checkpoint']}\")\n",
    "    pretrainedCkpt = load_checkpoint(args['resume_checkpoint'], device = device)\n",
    "    \n",
    "    # Create model with pretrained hyperparameters\n",
    "    pretrainedHParams = pretrainedCkpt[\"hyper_parameters\"]\n",
    "    plModel = PLBeatThis(\n",
    "        spect_dim       = pretrainedHParams.get(\"spect_dim\", 128),\n",
    "        fps             = pretrainedHParams.get(\"fps\", 50),\n",
    "        transformer_dim = pretrainedHParams.get(\"transformer_dim\", args['transformer_dim']),\n",
    "        ff_mult         = pretrainedHParams.get(\"ff_mult\", 4),\n",
    "        n_layers        = pretrainedHParams.get(\"n_layers\", args['n_layers'],),\n",
    "        stem_dim        = pretrainedHParams.get(\"stem_dim\", 32),\n",
    "        dropout         = dropout,\n",
    "        lr              = args['lr'],\n",
    "        weight_decay    = args['weight_decay'],\n",
    "        pos_weights     = pos_weights,\n",
    "        head_dim        = pretrainedHParams.get(\"head_dim\", 32),\n",
    "        loss_type       = args['loss'],\n",
    "        warmup_steps    = args['warmup_steps'],\n",
    "        max_epochs      = args['max_epochs'],\n",
    "        use_dbn         = args['dbn'],\n",
    "        eval_trim_beats = args['eval_trim_beats'],\n",
    "        sum_head        = pretrainedHParams.get(\"sum_head\", args['sum_head']),\n",
    "        partial_transformers    = pretrainedHParams.get(\"partial_transformers\", args['partial_transformers']),\n",
    "    )\n",
    "\n",
    "    # Load pretrained weights\n",
    "    plModel.load_state_dict(pretrainedCkpt[\"state_dict\"], strict = False)\n",
    "    print(\"Successfully loaded pretrained weights\")\n",
    "    \n",
    "else:\n",
    "    plModel = PLBeatThis(\n",
    "        spect_dim = 128,\n",
    "        fps       = 50,\n",
    "        transformer_dim = args['transformer_dim'],\n",
    "        ff_mult   = 4,\n",
    "        n_layers  = args['n_layers'],\n",
    "        stem_dim  = 32,\n",
    "        dropout   = dropout,\n",
    "        lr        = args['lr'],\n",
    "        weight_decay    = args['weight_decay'],\n",
    "        pos_weights     = pos_weights,\n",
    "        head_dim  = 32,\n",
    "        loss_type       = args['loss'],\n",
    "        warmup_steps    = args['warmup_steps'],\n",
    "        max_epochs      = args['max_epochs'],\n",
    "        use_dbn         = args['dbn'],\n",
    "        eval_trim_beats = args['eval_trim_beats'],\n",
    "        sum_head  = args['sum_head'],\n",
    "        partial_transformers = args['partial_transformers'],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for part in args['compile']:\n",
    "    if hasattr(plModel.model, part):\n",
    "        setattr(plModel.model, part, torch.compile(getattr(plModel.model, part)))\n",
    "        print(\"Will compile model\", part)\n",
    "    else:\n",
    "        raise ValueError(\"The model is missing the part\", part, \"to compile\")\n",
    "\n",
    "callbacks = [LearningRateMonitor(logging_interval=\"step\")]\n",
    "# save only the last model\n",
    "callbacks.append(\n",
    "    ModelCheckpoint(  # pyright: ignore[reportArgumentType]\n",
    "        every_n_epochs = 1,\n",
    "        dirpath        = str(checkpointDir),\n",
    "        filename       = f\"{args['name']} S{args['seed']} {params_str}\".strip(),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    max_epochs  = args['max_epochs'],\n",
    "    accelerator = \"auto\",\n",
    "    devices     = [args['gpu']],\n",
    "    num_sanity_val_steps    = 1,\n",
    "    logger      = logger,\n",
    "    callbacks   = callbacks,  # pyright: ignore[reportArgumentType]\n",
    "    log_every_n_steps       = 1,\n",
    "    precision   = \"16-mixed\",\n",
    "    accumulate_grad_batches = args['accumulate_grad_batches'],\n",
    "    check_val_every_n_epoch = args['val_frequency'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit( plModel, datamodule)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
